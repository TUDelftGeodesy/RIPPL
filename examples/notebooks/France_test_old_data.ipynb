{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this notebook we will explain the RIPPL workflow based on a study case about the earthquake on one of the islands\n",
    "of Hawaii on May 4th 2018\n",
    "\n",
    "In the first block we define the study area. There are different options to do so.\n",
    "1. Create a shapefile (this can be done using ArcGIS or QGIS software)\n",
    "2. Create a kml file using google earth > https://www.google.com/earth/\n",
    "3. Create a geojson using > http://geojson.io\n",
    "4. Create a list of coordinates in lat/lon coordinate pairs. This is what we will do here. \n",
    "\n",
    "To show the study area it is possible to run both google earth and geojson within the notebook.\n",
    "\n",
    "For further background on this event you can read:\n",
    "https://en.wikipedia.org/wiki/2018_Hawaii_earthquake\n",
    "\n",
    "Following image shows the earthquake strength (credits: USGS 2018):\n",
    "![Image of Hawaii earthquake strength](https://en.wikipedia.org/wiki/2018_Hawaii_earthquake#/media/File:2018_Hawaii_earthquake.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import script to embed geolocation.\n",
    "from IPython.display import HTML\n",
    "import datetime\n",
    "import sys\n",
    "import shutil\n",
    "import numpy as np\n",
    "import os\n",
    "sys.path.extend(['/Users/gertmulder/software/rippl_main'])\n",
    "\n",
    "import rippl\n",
    "\n",
    "from rippl.orbit_geometry.read_write_shapes import ReadWriteShapes\n",
    "from rippl.SAR_sensors.sentinel.sentinel_download import DownloadSentinel\n",
    "from rippl.processing_templates.general_sentinel_1 import GeneralPipelines\n",
    "\n",
    "France_shape = [(7.20, 43.75), (7.50, 43.75), (7.50, 43.90), (7.20, 43.90), (7.20, 43.75)]\n",
    "study_area = ReadWriteShapes()\n",
    "study_area(France_shape)\n",
    "\n",
    "geojson = study_area.shape\n",
    "\n",
    "# Try to do the same by creating a shapefile with QGIS, geojson online or a .kml file in google earth.\n",
    "# study_area.read_kml(kml_path)\n",
    "# study_area.read_geo_json(geojson_path)\n",
    "# study_area.read_shapefile(shapefile_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After selection of the right track we can start the actual download of the images. In our case we use track 88.\n",
    "\n",
    "This will download our data automatically to our radar database. Additionally, it will download the precise orbit files.\n",
    "These files are created within a few weeks after the data acquisition and define the satellite orbit within a few cm\n",
    "accuracy. These orbits are necessary to accurately define the positions of the radar pixels on the ground later on\n",
    "in the processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading available orbit data files\n",
      "Finished loading date files\n",
      "S1A_IW_SLC__1SDV_20160310T172230_20160310T172257_010310_00F422_204E.zip\n",
      "S1A_IW_SLC__1SDV_20160310T172205_20160310T172232_010310_00F422_CB7A.zip\n",
      "S1A_IW_SLC__1SDV_20160403T172231_20160403T172258_010660_00FE19_7395.zip\n",
      "S1A_IW_SLC__1SDV_20160403T172206_20160403T172233_010660_00FE19_79EB.zip\n",
      "S1A_IW_SLC__1SDV_20160322T172219_20160322T172246_010485_00F908_2AD9.zip\n",
      "S1A_IW_SLC__1SDV_20160415T172214_20160415T172241_010835_01035A_07FA.zip\n",
      "Selected images:\n",
      "S1A_IW_SLC__1SDV_20160310T172230_20160310T172257_010310_00F422_204E.zip\n",
      "S1A_IW_SLC__1SDV_20160310T172205_20160310T172232_010310_00F422_CB7A.zip\n",
      "S1A_IW_SLC__1SDV_20160403T172231_20160403T172258_010660_00FE19_7395.zip\n",
      "S1A_IW_SLC__1SDV_20160403T172206_20160403T172233_010660_00FE19_79EB.zip\n",
      "S1A_IW_SLC__1SDV_20160322T172219_20160322T172246_010485_00F908_2AD9.zip\n",
      "S1A_IW_SLC__1SDV_20160415T172214_20160415T172241_010835_01035A_07FA.zip\n",
      "\n",
      "Reading precise and restituted orbit database\n",
      "Reading swath .xml files for new images:\n",
      "Read meta_data s1a-iw1-slc-vv-20160310t172231-20160310t172256-010310-00f422-004.xml\n",
      "Read meta_data s1a-iw2-slc-vv-20160310t172232-20160310t172257-010310-00f422-005.xml\n",
      "Read meta_data s1a-iw3-slc-vv-20160310t172230-20160310t172255-010310-00f422-006.xml\n",
      "Read meta_data s1a-iw1-slc-vv-20160310t172206-20160310t172231-010310-00f422-004.xml\n",
      "Read meta_data s1a-iw2-slc-vv-20160310t172207-20160310t172232-010310-00f422-005.xml\n",
      "Read meta_data s1a-iw3-slc-vv-20160310t172205-20160310t172231-010310-00f422-006.xml\n",
      "Read meta_data s1a-iw1-slc-vv-20160403t172232-20160403t172257-010660-00fe19-004.xml\n",
      "Read meta_data s1a-iw2-slc-vv-20160403t172233-20160403t172258-010660-00fe19-005.xml\n",
      "Read meta_data s1a-iw3-slc-vv-20160403t172231-20160403t172256-010660-00fe19-006.xml\n",
      "Read meta_data s1a-iw1-slc-vv-20160403t172207-20160403t172232-010660-00fe19-004.xml\n",
      "Read meta_data s1a-iw2-slc-vv-20160403t172208-20160403t172233-010660-00fe19-005.xml\n",
      "Read meta_data s1a-iw3-slc-vv-20160403t172206-20160403t172231-010660-00fe19-006.xml\n",
      "Read meta_data s1a-iw1-slc-vv-20160322t172221-20160322t172246-010485-00f908-004.xml\n",
      "Read meta_data s1a-iw2-slc-vv-20160322t172219-20160322t172244-010485-00f908-005.xml\n",
      "Read meta_data s1a-iw3-slc-vv-20160322t172220-20160322t172245-010485-00f908-006.xml\n",
      "Read meta_data s1a-iw1-slc-vv-20160415t172216-20160415t172241-010835-01035a-004.xml\n",
      "Read meta_data s1a-iw2-slc-vv-20160415t172214-20160415t172239-010835-01035a-005.xml\n",
      "Read meta_data s1a-iw3-slc-vv-20160415t172215-20160415t172240-010835-01035a-006.xml\n",
      "\n",
      "Master slice list already loaded!\n",
      "Assign found bursts to data_stack. These can be part of current data_stack already!\n",
      "Assigned master burst slice_500_swath_1 at 2016-03-22\n",
      "Assigned master burst slice_501_swath_1 at 2016-03-22\n",
      "Assigned master burst slice_502_swath_1 at 2016-03-22\n",
      "Assigned master burst slice_500_swath_2 at 2016-03-22\n",
      "Assigned master burst slice_501_swath_2 at 2016-03-22\n",
      "\n",
      "Assigned slave burst slice_501_swath_1 at 2016-03-10\n",
      "Assigned slave burst slice_502_swath_1 at 2016-03-10\n",
      "Assigned slave burst slice_501_swath_2 at 2016-03-10\n",
      "Assigned slave burst slice_500_swath_1 at 2016-03-10\n",
      "Assigned slave burst slice_500_swath_2 at 2016-03-10\n",
      "Assigned slave burst slice_501_swath_1 at 2016-04-03\n",
      "Assigned slave burst slice_502_swath_1 at 2016-04-03\n",
      "Assigned slave burst slice_501_swath_2 at 2016-04-03\n",
      "Assigned slave burst slice_500_swath_1 at 2016-04-03\n",
      "Assigned slave burst slice_500_swath_2 at 2016-04-03\n",
      "Assigned slave burst slice_500_swath_1 at 2016-04-15\n",
      "Assigned slave burst slice_501_swath_1 at 2016-04-15\n",
      "Assigned slave burst slice_502_swath_1 at 2016-04-15\n",
      "Assigned slave burst slice_500_swath_2 at 2016-04-15\n",
      "Assigned slave burst slice_501_swath_2 at 2016-04-15\n",
      "Initializing master bursts 0 to 5 from total of 5\n",
      "Initializing slave bursts 0 to 15 from total of 15\n"
     ]
    }
   ],
   "source": [
    "# Track and data type of Sentinel data\n",
    "mode = 'IW'\n",
    "product_type = 'SLC'\n",
    "polarisation = 'VV'\n",
    "\n",
    "from rippl.processing_templates.general_sentinel_1 import GeneralPipelines\n",
    "\n",
    "# Create the list of the 4 different stacks.\n",
    "track_no = 88\n",
    "stack_name = 'France_old_data'\n",
    "tmp_directory = '/Users/gertmulder/SAR_tmp'\n",
    "if not os.path.exists(tmp_directory):\n",
    "    os.mkdir(tmp_directory)\n",
    "coreg_tmp_directory = '/Users/gertmulder/SAR_tmp'\n",
    "if not os.path.exists(coreg_tmp_directory):\n",
    "    os.mkdir(coreg_tmp_directory)\n",
    "ml_grid_tmp_directory = '/Users/gertmulder/SAR_tmp'\n",
    "if not os.path.exists(ml_grid_tmp_directory):\n",
    "    os.mkdir(ml_grid_tmp_directory)\n",
    "\n",
    "# For every track we have to select a master date. This is based on the search results earlier.\n",
    "# Choose the date with the lowest coverage to create an image with only the overlapping parts.\n",
    "start_date = datetime.datetime(year=2016, month=3, day=3)\n",
    "end_date = datetime.datetime(year=2016, month=4, day=18)\n",
    "master_date = datetime.datetime(year=2016, month=3, day=22)\n",
    "\n",
    "# Number of processes for parallel processing. Make sure that for every process at least 2GB of RAM is available\n",
    "no_processes = 4\n",
    "\n",
    "s1_processing = GeneralPipelines(processes=no_processes)\n",
    "s1_processing.download_sentinel_data(start_date=start_date, end_date=end_date, track=track_no,\n",
    "                                           polarisation=polarisation, shapefile=study_area.shape, data=True, source='ASF')\n",
    "s1_processing.create_sentinel_stack(start_date=start_date, end_date=end_date, master_date=master_date, cores=no_processes,\n",
    "                                          track=track_no,stack_name=stack_name, polarisation=polarisation,\n",
    "                                          shapefile=study_area.shape, mode=mode, product_type=product_type)\n",
    "\n",
    "# Finally load the stack itself. If you want to skip the download step later, run this line before other steps!\n",
    "s1_processing.read_stack(start_date=start_date, end_date=end_date, stack_name=stack_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To define the location of the radar pixels on the ground we need the terrain elevation. Although it is possible to \n",
    "derive terrain elevation from InSAR data, our used Sentinel-1 dataset is not suitable for this purpose. Therefore, we\n",
    "download data from an external source to create a digital elevation model (DEM). In our case we use SRTM data. \n",
    "\n",
    "However, to find the elevation of the SAR data grid, we have to resample the data to the radar grid first to make it\n",
    "usable. This is done in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Some basic settings for DEM creation.\n",
    "dem_buffer = 0.1        # Buffer around radar image where DEM data is downloaded\n",
    "dem_rounding = 0.1      # Rounding of DEM size in degrees\n",
    "dem_type = 'SRTM1'      # DEM type of data we download (SRTM1, SRTM3 and TanDEM-X are supported)\n",
    "\n",
    "# Define both the coordinate system of the full radar image and imported DEM\n",
    "s1_processing.create_radar_coordinates()\n",
    "s1_processing.create_dem_coordinates(dem_type=dem_type)\n",
    "\n",
    "# Download external DEM\n",
    "s1_processing.download_external_dem(dem_type=dem_type, buffer=dem_buffer, rounding=dem_rounding, n_processes=no_processes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Using the obtained elevation model the exact location of the radar pixels in cartesian (X,Y,Z) and geographic (Lat/Lon)\n",
    "can be derived. This is only done for the master or reference image. This process is referred to as geocoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pipeline block 1 out of 1\n",
      "Skipping processing. Process already finished\n",
      "Skipping processing. Process already finished\n",
      "Skipping processing. Process already finished\n",
      "Skipping processing. Process already finished\n",
      "Skipping processing. Process already finished\n",
      "Processing pipeline block 1 out of 1\n",
      "Skipping processing. Process already finished\n",
      "Skipping processing. Process already finished\n",
      "Skipping processing. Process already finished\n",
      "Skipping processing. Process already finished\n",
      "Skipping processing. Process already finished\n",
      "Concatenated dataset already exists. If you want to overwrite set overwrite to True\n",
      "Concatenated dataset already exists. If you want to overwrite set overwrite to True\n",
      "Concatenated dataset already exists. If you want to overwrite set overwrite to True\n",
      "Concatenated dataset already exists. If you want to overwrite set overwrite to True\n",
      "Concatenated dataset already exists. If you want to overwrite set overwrite to True\n"
     ]
    }
   ],
   "source": [
    "# Geocoding of image\n",
    "s1_processing.geocoding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The information from the geocoding can directly be used to find the location of the master grid pixels in the slave\n",
    "grid images. This process is called coregistration. Because the orbits are not exactly the same with every satellite \n",
    "overpass but differ hundreds to a few thousand meters every overpass, the grids are slightly shifted with respect to \n",
    "each other. These shift are referred to as the spatial baseline of the images. To correctly overlay the master and slave\n",
    "images the software coregisters and resamples to the master grid.\n",
    "\n",
    "To do so the following steps are done:\n",
    "1. Coregistration of slave to master image\n",
    "2. Deramping the doppler effects due to TOPs mode of Sentinel-1 satellite\n",
    "3. Resampling of slave image\n",
    "4. Reramping resampled slave image.\n",
    "\n",
    "Due to the different orbits of the master and slave image, the phase of the radar signal is also shifted. We do not \n",
    "know the exact shift of the two image, but using the geometry of the two images we can estimate the shift of the phase\n",
    "between different pixels. Often this shift is split in two contributions:\n",
    "1. The flat earth phase. This phase is the shift in the case the earth was a perfect ellipsoid\n",
    "2. The topographic phase. This is the phase shift due to the topography on the ground.\n",
    "In our processing these two corrections are done in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pipeline block 1 out of 1\n",
      "Skipping processing. Process already finished\n",
      "Skipping processing. Process already finished\n",
      "Skipping processing. Process already finished\n",
      "Skipping processing. Process already finished\n",
      "Skipping processing. Process already finished\n",
      "Skipping processing. Process already finished\n",
      "Skipping processing. Process already finished\n",
      "Skipping processing. Process already finished\n",
      "Skipping processing. Process already finished\n",
      "Skipping processing. Process already finished\n"
     ]
    }
   ],
   "source": [
    "# Next step applies resampling and phase correction in one step.\n",
    "# Polarisation\n",
    "\n",
    "# Because with the geometric coregistrtation we load the X,Y,Z files of the main image for every calculation it can\n",
    "# be beneficial to load them to a fast temporary disk. (If enough space you should load them to memory disk)\n",
    "s1_processing.geometric_coregistration_resampling(polarisation=polarisation, output_phase_correction=True,\n",
    "                                                  coreg_tmp_directory=coreg_tmp_directory, tmp_directory=tmp_directory, baselines=False, height_to_phase=True)\n",
    "shutil.rmtree(coreg_tmp_directory)\n",
    "os.mkdir(coreg_tmp_directory)\n",
    "shutil.rmtree(tmp_directory)\n",
    "os.mkdir(tmp_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we can create calibrated amplitudes, interferograms and coherences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using predefined grid size for concatenation of slices\n",
      "Concatenated dataset already exists. If you want to overwrite set overwrite to True\n",
      "Processing pipeline block 1 out of 1\n",
      "Processing pipeline block 1 out of 1\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "Pipeline processing for /Users/gertmulder/SAR_processing/radar_data_stacks/sentinel1/France_old_data/20160310 failed.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRemoteTraceback\u001B[0m                           Traceback (most recent call last)",
      "\u001B[0;31mRemoteTraceback\u001B[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/gertmulder/software/rippl_main/rippl/run_parallel.py\", line 72, in run_parallel\n    process.load_input_data_files(tmp_directory=dat['tmp_directory'], coreg_tmp_directory=dat['coreg_tmp_directory'])\n  File \"/Users/gertmulder/software/rippl_main/rippl/meta_data/process.py\", line 660, in load_input_data_files\n    raise FileNotFoundError('Data on disk not found.')\nFileNotFoundError: Data on disk not found.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/gertmulder/.conda/envs/rippl_master/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n    result = (True, func(*args, **kwds))\n  File \"/Users/gertmulder/software/rippl_main/rippl/run_parallel.py\", line 91, in run_parallel\n    raise BrokenPipeError('Pipeline processing for ' + process.out_processing_image.folder + ' failed.')\nBrokenPipeError: Pipeline processing for /Users/gertmulder/SAR_processing/radar_data_stacks/sentinel1/France_old_data/20160310 failed.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mBrokenPipeError\u001B[0m                           Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-6-34904f0e660e>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     36\u001B[0m             \u001B[0ms1_processing\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreate_ml_coordinates\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstandard_type\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'oblique_mercator'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdx\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbuffer\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrounding\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     37\u001B[0m             \u001B[0ms1_processing\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprepare_multilooking_grid\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpolarisation\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 38\u001B[0;31m             \u001B[0ms1_processing\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreate_calibrated_amplitude_multilooked\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpolarisation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcoreg_tmp_directory\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcoreg_tmp_directory\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtmp_directory\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtmp_directory\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     39\u001B[0m             \u001B[0ms1_processing\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreate_output_tiffs_amplitude\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/software/rippl_main/rippl/processing_templates/general_sentinel_1.py\u001B[0m in \u001B[0;36mcreate_calibrated_amplitude_multilooked\u001B[0;34m(self, polarisation, block_orientation, tmp_directory, coreg_tmp_directory)\u001B[0m\n\u001B[1;32m    650\u001B[0m                 CalibratedAmplitudeMultilook(polarisation=pol, in_coor=self.radar_coor, out_coor=self.full_ml_coor,\n\u001B[1;32m    651\u001B[0m                                        slave='slave', coreg_master='coreg_master', batch_size=50000000), True, True)\n\u001B[0;32m--> 652\u001B[0;31m             \u001B[0mcreate_multilooked_amp\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    653\u001B[0m             \u001B[0mcreate_multilooked_amp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msave_processing_results\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    654\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/software/rippl_main/rippl/pipeline.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    154\u001B[0m                 \u001B[0;32mwith\u001B[0m \u001B[0mget_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"spawn\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPool\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprocesses\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprocesses\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmaxtasksperchild\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mpool\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    155\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 156\u001B[0;31m                     \u001B[0;32mfor\u001B[0m \u001B[0mjson_out\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpool\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mimap_unordered\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrun_parallel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mblock_pipelines\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mchunksize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    157\u001B[0m                         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson_dicts\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjson_out\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    158\u001B[0m                         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson_files\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjson_out\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/rippl_master/lib/python3.7/multiprocessing/pool.py\u001B[0m in \u001B[0;36mnext\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    746\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0msuccess\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    747\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 748\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    749\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    750\u001B[0m     \u001B[0m__next__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnext\u001B[0m                    \u001B[0;31m# XXX\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mBrokenPipeError\u001B[0m: Pipeline processing for /Users/gertmulder/SAR_processing/radar_data_stacks/sentinel1/France_old_data/20160310 failed."
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the images in blocks to temporary disk (or not if only coreg data is loaded to temp disk)\n",
    "temporal_baseline = 30\n",
    "min_timespan = temporal_baseline * 2\n",
    "# Every process can only run 1 multilooking job. Therefore, in the case of amplitude calculation the number of processes\n",
    "# is limited too the number of images loaded.\n",
    "amp_processing_efficiency = 0.5\n",
    "effective_timespan = np.maximum(no_processes * 6 * amp_processing_efficiency, min_timespan)\n",
    "\n",
    "no_days = datetime.timedelta(days=int(effective_timespan / 2))\n",
    "if no_days < (end_date - start_date):\n",
    "    step_date = start_date\n",
    "    step_dates = []\n",
    "    while step_date < end_date:\n",
    "        step_dates.append(step_date)\n",
    "        step_date += no_days\n",
    "    step_dates.append(end_date)\n",
    "\n",
    "    start_dates = step_dates[:-2]\n",
    "    end_dates = step_dates[2:]\n",
    "else:\n",
    "    end_dates = [end_date]\n",
    "    start_dates = [start_date]\n",
    "\n",
    "\n",
    "if not isinstance(polarisation, list):\n",
    "    pol = [polarisation]\n",
    "else:\n",
    "    pol = polarisation\n",
    "\n",
    "for start_date, end_date in zip(start_dates, end_dates):\n",
    "    s1_processing.read_stack(start_date=start_date, end_date=end_date, stack_name=stack_name)\n",
    "    # We split the different polarisation to limit the number of files in the temporary folder.\n",
    "    for p in pol:\n",
    "        for dist in [50, 100, 200]:\n",
    "            # The actual creation of the calibrated amplitude images\n",
    "            s1_processing.create_ml_coordinates(standard_type='oblique_mercator', dx=dist, dy=dist, buffer=0, rounding=0)\n",
    "            s1_processing.prepare_multilooking_grid(polarisation)\n",
    "            s1_processing.create_calibrated_amplitude_multilooked(polarisation, coreg_tmp_directory=coreg_tmp_directory, tmp_directory=tmp_directory)\n",
    "            s1_processing.create_output_tiffs_amplitude()\n",
    "\n",
    "            s1_processing.create_ifg_network(temporal_baseline=temporal_baseline)\n",
    "            s1_processing.create_interferogram_multilooked(polarisation, coreg_tmp_directory=ml_grid_tmp_directory, tmp_directory=tmp_directory)\n",
    "            s1_processing.create_coherence_multilooked(polarisation, coreg_tmp_directory=ml_grid_tmp_directory, tmp_directory=tmp_directory)\n",
    "\n",
    "            # Create output geotiffs\n",
    "            s1_processing.create_output_tiffs_coherence_ifg()\n",
    "\n",
    "            # Create lat/lon/incidence angle/DEM for multilooked grid.\n",
    "            s1_processing.create_geometry_mulitlooked(baselines=True, height_to_phase=True)\n",
    "            s1_processing.create_output_tiffs_geometry()\n",
    "\n",
    "            # The coreg temp directory will only contain the loaded input lines/pixels to do the multilooking. These\n",
    "            # files will be called by every process so it can be usefull to load them in memory the whole time.\n",
    "            # If not given, these files will be loaded in the regular tmp folder.\n",
    "            if coreg_tmp_directory:\n",
    "                if os.path.exists(ml_grid_tmp_directory):\n",
    "                    shutil.rmtree(ml_grid_tmp_directory)\n",
    "                    os.mkdir(ml_grid_tmp_directory)\n",
    "\n",
    "        for dist in [50, 100, 200]:\n",
    "            # The actual creation of the calibrated amplitude images\n",
    "            s1_processing.create_ml_coordinates(standard_type='UTM', dx=dist, dy=dist, buffer=0, rounding=0)\n",
    "            s1_processing.prepare_multilooking_grid(polarisation)\n",
    "            s1_processing.create_calibrated_amplitude_multilooked(polarisation, coreg_tmp_directory=coreg_tmp_directory, tmp_directory=tmp_directory)\n",
    "            s1_processing.create_output_tiffs_amplitude()\n",
    "\n",
    "            s1_processing.create_ifg_network(temporal_baseline=temporal_baseline)\n",
    "            s1_processing.create_interferogram_multilooked(polarisation, coreg_tmp_directory=ml_grid_tmp_directory, tmp_directory=tmp_directory)\n",
    "            s1_processing.create_coherence_multilooked(polarisation, coreg_tmp_directory=ml_grid_tmp_directory, tmp_directory=tmp_directory)\n",
    "\n",
    "            # Create output geotiffs\n",
    "            s1_processing.create_output_tiffs_coherence_ifg()\n",
    "\n",
    "            # Create lat/lon/incidence angle/DEM for multilooked grid.\n",
    "            s1_processing.create_geometry_mulitlooked(baselines=True, height_to_phase=True)\n",
    "            s1_processing.create_output_tiffs_geometry()\n",
    "\n",
    "            # The coreg temp directory will only contain the loaded input lines/pixels to do the multilooking. These\n",
    "            # files will be called by every process so it can be usefull to load them in memory the whole time.\n",
    "            # If not given, these files will be loaded in the regular tmp folder.\n",
    "            if coreg_tmp_directory:\n",
    "                if os.path.exists(ml_grid_tmp_directory):\n",
    "                    shutil.rmtree(ml_grid_tmp_directory)\n",
    "                    os.mkdir(ml_grid_tmp_directory)\n",
    "\n",
    "        for dist in [0.0005, 0.001, 0.002]:\n",
    "            # The actual creation of the calibrated amplitude images\n",
    "            s1_processing.create_ml_coordinates(dlat=dist, dlon=dist, coor_type='geographic', buffer=0, rounding=0)\n",
    "            s1_processing.prepare_multilooking_grid(polarisation)\n",
    "            s1_processing.create_calibrated_amplitude_multilooked(polarisation, coreg_tmp_directory=coreg_tmp_directory, tmp_directory=tmp_directory)\n",
    "            s1_processing.create_output_tiffs_amplitude()\n",
    "\n",
    "            s1_processing.create_ifg_network(temporal_baseline=temporal_baseline)\n",
    "            s1_processing.create_interferogram_multilooked(polarisation, coreg_tmp_directory=ml_grid_tmp_directory, tmp_directory=tmp_directory)\n",
    "            s1_processing.create_coherence_multilooked(polarisation, coreg_tmp_directory=ml_grid_tmp_directory, tmp_directory=tmp_directory)\n",
    "\n",
    "            # Create output geotiffs\n",
    "            s1_processing.create_output_tiffs_coherence_ifg()\n",
    "\n",
    "            # Create lat/lon/incidence angle/DEM for multilooked grid.\n",
    "            s1_processing.create_geometry_mulitlooked(baselines=True, height_to_phase=True)\n",
    "            s1_processing.create_output_tiffs_geometry()\n",
    "\n",
    "            # The coreg temp directory will only contain the loaded input lines/pixels to do the multilooking. These\n",
    "            # files will be called by every process so it can be usefull to load them in memory the whole time.\n",
    "            # If not given, these files will be loaded in the regular tmp folder.\n",
    "            if coreg_tmp_directory:\n",
    "                if os.path.exists(ml_grid_tmp_directory):\n",
    "                    shutil.rmtree(ml_grid_tmp_directory)\n",
    "                    os.mkdir(ml_grid_tmp_directory)\n",
    "\n",
    "        if tmp_directory:\n",
    "            if os.path.exists(tmp_directory):\n",
    "                shutil.rmtree(tmp_directory)\n",
    "                os.mkdir(tmp_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}